{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP starting kit on the Human Locomotion data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publication\n",
    "\n",
    "Truong, C., Barrois-Müller, R., Moreau, T., Provost, C., Vienne-Jumeau, A., Moreau, A., Vidal, P.-P., Vayatis, N., Buffat, S., Yelnik, A., Ricard, D., & Oudre, L. (2019). A data set for the study of human locomotion with inertial measurements units. Image Processing On Line (IPOL), 9. [[doi]](https://doi.org/10.5201/ipol.2019.265) [[pdf]](http://deepcharles.github.io/files/ipol-walk-data-2019.pdf) [[online demo]](http://ipolcore.ipol.im/demo/clientApp/demo.html?id=265)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Python >=3.7\n",
    "- numpy\n",
    "- scipy\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- matplolib\n",
    "- jupyter\n",
    "- ramp-workflow\n",
    "\n",
    "The following cell will install if necessary the missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from scikit-learn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from scikit-learn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from scikit-learn) (0.17.0)\n",
      "Collecting https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
      "  Using cached https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
      "Requirement already satisfied (use --upgrade to upgrade): ramp-workflow==0.4.0.dev0 from https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages\n",
      "Requirement already satisfied: numpy in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (1.19.4)\n",
      "Requirement already satisfied: scipy in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (1.5.4)\n",
      "Requirement already satisfied: pandas in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (1.1.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (0.23.2)\n",
      "Requirement already satisfied: joblib in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (0.17.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (1.6.0)\n",
      "Requirement already satisfied: click in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from ramp-workflow==0.4.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from pandas->ramp-workflow==0.4.0.dev0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from pandas->ramp-workflow==0.4.0.dev0) (2020.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from scikit-learn>=0.22->ramp-workflow==0.4.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Sara/.virtualenvs/ramp/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->ramp-workflow==0.4.0.dev0) (1.15.0)\n",
      "Building wheels for collected packages: ramp-workflow\n",
      "  Building wheel for ramp-workflow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ramp-workflow: filename=ramp_workflow-0.4.0.dev0-py3-none-any.whl size=125438 sha256=222a724a77b980794384dcdf93b266ac9c976cfd8c32f2314b55556f8b95c4cb\n",
      "  Stored in directory: /private/var/folders/wn/78t_mc615rs_qx7hzwzfgmp00000gn/T/pip-ephem-wheel-cache-jiztnzk2/wheels/62/9d/79/0bf798131db514100fbb090d0f57c4427ed8ac079eee994227\n",
      "Successfully built ramp-workflow\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "# Install ramp-workflow from the master branch on GitHub.\n",
    "!{sys.executable} -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download the download the data, execute the following command:\n",
    "\n",
    "```\n",
    "python download_data.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Context**\n",
    "\n",
    "The study of human gait is a central problem in medical research with far-reaching consequences in the public health domain.\n",
    "This complex mechanism can be altered by a wide range of pathologies (such as Parkinson’s disease, arthritis, stroke,...), often resulting in a significant loss of autonomy and an increased risk of fall.\n",
    "Understanding the influence of such medical disorders on a subject's gait would greatly facilitate early detection and prevention of those possibly harmful situations.\n",
    "To address these issues, clinical and bio-mechanical researchers have worked to objectively quantify gait characteristics.\n",
    "\n",
    "\n",
    "Among the gait features that have proved their relevance in a medical context, several are linked to the notion of step (step duration, variation in step length, etc.), which can be seen as the core atom of the locomotion process.\n",
    "Many algorithms have therefore been developed to automatically (or semi-automatically) detect gait events (such as heel-strikes, heel-off, etc.) from accelerometer/gyrometer signals.\n",
    "\n",
    "Most of the time, the algorithms used for step detection are dedicated to a specific population (healthy subjects, elderly subjects, Parkinson patients, etc.) and only a few publications deal with heterogeneous populations composed of several types of subjects.\n",
    "Another limit to existing algorithms is that they often focus on locomotion in established regime (once the subject has initiated its gait) and do not deal with steps during U-turn, gait initiation or gait termination.\n",
    "Yet, initiation and termination steps are particularly sensitive to pathological states.\n",
    "For example, the first step of Parkinsonian patients has been described as slower and smaller that the first step of age-matched subjects.\n",
    "U-turn steps are also interesting since 45% of daily living walking is made up of turning steps, and when compared to straight-line walking, turning has been emphasized as a high-risk fall situation.\n",
    "This argues for reliable algorithms that could detect initiation, termination and turning steps in both healthy and pathological subjects.\n",
    "\n",
    "\n",
    "**Step detection**\n",
    "\n",
    "The objective is to recognize the **start and end times of footsteps** contained in accelerometer and gyrometer signals recorded with Inertial Measurement Units (IMUs).\n",
    "\n",
    "\n",
    "\n",
    "## Data collection and clinical protocol\n",
    "\n",
    "#### Participants\n",
    "\n",
    "The data was collected between April 2014 and October 2015 by monitoring healthy (control) subjects and patients from several medical departments (see [publication](#Publication) for more information).\n",
    "Participants are divided into three groups depending on their impairment:\n",
    "- **Healthy** subjects had no known medical impairment.\n",
    "- The **orthopedic group** is composed of 2 cohorts of distinct pathologies: lower limb osteoarthrosis and cruciate ligament injury.\n",
    "- The **neurological group** is composed of 4 cohorts: hemispheric stroke, Parkinson's disease, toxic peripheral neuropathy and radiation induced leukoencephalopathy.\n",
    "\n",
    "Note that certain participants were recorded on multiple occasions, therefore several trials may correspond to the same person.\n",
    "In the training set and in the testing set, the proportion of trials coming from the \"healthy\", \"orthopedic\" and \"neurological\" groups is roughly the same, 24%, 24% and 52% respectively.\n",
    "\n",
    "#### Protocol and equipment\n",
    "\n",
    "All subjects underwent the same protocol described below. First, a IMU (Inertial Measurement Unit) that recorded accelerations and angular velocities was attached to each foot.\n",
    "All signals have been acquired at 100 Hz with two brands of IMUs: XSens&trade; and Technoconcept&reg;.\n",
    "One brand of IMU was attached to the dorsal face of each foot.\n",
    "(Both feet wore the same brand.)\n",
    "After sensor fixation, participants were asked to perform the following sequence of activities:\n",
    "- stand for 6 s,\n",
    "- walk 10 m at preferred walking speed on a level surface to a previously shown turn point,\n",
    "- turn around (without previous specification of a turning side),\n",
    "- walk back to the starting point,\n",
    "- stand for 2 s.\n",
    "\n",
    "Subjects walked at their comfortable speed with their shoes and without walking aid.\n",
    "This protocol is schematically illustrated in the following figure.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/protocol-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Each IMU records its acceleration and angular velocity in the $(X, Y, Z, V)$ set of axes defined in the following figure.\n",
    "The $V$ axis is aligned with gravity, while the $X$, $Y$ and $Z$ axes are attached to the sensor.\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-photo.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/sensor-position.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Step detection in a clinical context\n",
    "\n",
    "The following schema describes how step detection methods are integrated in a clinical context.\n",
    "<br/><br/>\n",
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/human_locomotion/master/images/step-detection-schema.png\" width=\"500px\">\n",
    "</div>\n",
    "\n",
    "(1) During a trial, sensors send their own acceleration and angular velocity to the physician's computer.\n",
    "\n",
    "(2) A software on the physician's computer synchronizes the data sent from both sensors and produces two multivariate signals (of same shape), each corresponding to a foot.\n",
    "\n",
    "\n",
    "A step detection procedure is applied on each signal to produce two lists of footsteps (one per foot/sensor).\n",
    "The numbers of left footsteps and right footsteps are not necessarily the same.\n",
    "Indeed, subjects often have a preferred foot to initiate and terminate a walk or a u-turn, resulting in one or more footsteps from this preferred foot.\n",
    "The starts and ends of footsteps are then used to create meaningful features to characterize the subject's gait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "During a trial, a subject executes the protocol described above.\n",
    "This produces two multivariates signals (one for each foot/sensor) and for each signal, a number of footsteps have be annotated.\n",
    "In addition, information (metadata) about the trial and participant are provided.\n",
    "All three elements (signal, step annotation and metadata) are detailled in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "from problem import get_train_data, get_test_data\n",
    "\n",
    "data_train, labels_train = get_train_data()\n",
    "data_test, labels_test = get_test_data()\n",
    "# print some information\n",
    "msg = f\"There are {len(data_train)} elements in the training set, and {len(data_test)} in the testing set.\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of either `data_train` or `data_test` is a signal collected from a single sensor (left or right foot) on a given trial, augmented with a set of metadata; `labels_train` or `labels_test` contain the footsteps that were manually annotated by medical experts from the sensor signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrative example of a sensor recording\n",
    "index = 60\n",
    "sensor_data, step_list = data_train[index], labels_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal\n",
    "\n",
    "Each IMU that the participants wore provided $\\mathbb{R}^{8}$-valued signals, sampled at 100 Hz.\n",
    "In this setting, each dimension is defined by the signal type (`A` for acceleration, `R` for angular velocity) and the axis (`X`, `Y`, `Z` or `V`).\n",
    "For instance, `RX` denotes the angular velocity around the `X`-axis.\n",
    "Accelerations are given in $m/s^2$ and angular velocities, in $deg/s$.\n",
    "The signal is available in the `.signal` attribute as a `Pandas` dataframe.\n",
    "\n",
    "Note that this multivariate signal originates from a single sensor.\n",
    "Therefore a single trial produces two signals (i.e. two training/testing instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signal is available in the `signal` attribute.\n",
    "sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]].plot(figsize=(10, 5))  # select the accelerations\n",
    "sensor_data.signal[[\"RX\", \"RY\", \"RZ\", \"RV\"]].plot(figsize=(10, 5))  # select the angular velocities\n",
    "\n",
    "sensor_data.signal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"flat part\" at the beginning of each dimension is the result of the participants standing still for a few\n",
    "seconds before walking (see [Protocol](#Protocol-and-equipment)).\n",
    "The same behaviour can be seen at the end of each dimension (often but not always), though for a quite smaller duration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Metadata\n",
    "A number of metadata (either numerical or categorical) are provided for each sensor recording, detailing the participant being monitored and the sensor position:\n",
    "\n",
    "- `trial_code`: unique identifier for the trial;\n",
    "- `age` (in years);\n",
    "- `gender`: male (\"M\") or female (\"F\");\n",
    "- `height` (in meters);\n",
    "- `weight` (in kilograms);\n",
    "- `bmi` (in kg/m2): body mass index;\n",
    "- `laterality`: subject's \"footedness\" or \"foot to kick a ball\" (\"Left\", \"Right\" or \"Ambidextrous\").\n",
    "- `sensor`: brand of the IMU used for the recording (“XSens” or “TCon”);\n",
    "- `pathology_group`: this variable takes value in {“Healthy”, “Orthopedic”, “Neurological”};\n",
    "- `is_control`: whether the subject is a control subject (\"Yes\" or \"No\");\n",
    "- `foot`: foot on which the sensor was attached (\"Left\" or \"Right\").\n",
    "\n",
    "These are accessible using the notation `sensor_data.<metadata>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f\"\"\"Metadata:\n",
    "age\\t\\t{sensor_data.age},\n",
    "gender\\t\\t{sensor_data.gender},\n",
    "height\\t\\t{sensor_data.height},\n",
    "weight\\t\\t{sensor_data.weight},\n",
    "bmi\\t\\t{sensor_data.bmi},\n",
    "laterality\\t{sensor_data.laterality},\n",
    "sensor\\t\\t{sensor_data.sensor},\n",
    "pathology_group\\t{sensor_data.pathology_group},\n",
    "trial code\\t{sensor_data.trial_code},\n",
    "foot\\t\\t{sensor_data.foot}.\"\"\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step annotation (the \"label\" to predict)\n",
    "Footsteps were manually annotated by specialists using a software that displayed the signals from the relevant sensor (left or right foot) and allowed the specialist to indicate the starts and ends of each step.\n",
    "\n",
    "A footstep is defined as the period during which the foot is moving.\n",
    "Footsteps are separated by periods when the foot is still and flat on the floor.\n",
    "Therefore, in our setting, a footstep starts with a heel-off and ends with the following toe-strike of the same foot.\n",
    "\n",
    "\n",
    "Footsteps (the \"label\" to predict from the signal) are contained in a list whose elements are list of two integers, the start and end indexes. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of footsteps and signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = f\"For the trial '{sensor_data.trial_code}', {len(step_list)} footsteps were annotated on the {sensor_data.foot.lower()} foot:\"\n",
    "print(msg)\n",
    "print(step_list)\n",
    "\n",
    "# Color the footsteps\n",
    "ax = sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]].plot(figsize=(10, 5))\n",
    "line_args = {\"linestyle\": \"--\", \"color\": \"k\"}\n",
    "for (start, end) in step_list:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor='g', alpha=0.3)\n",
    "\n",
    "ax = sensor_data.signal[[\"RX\", \"RY\", \"RZ\", \"RV\"]].plot(figsize=(10, 5))\n",
    "for (start, end) in step_list:\n",
    "    ax.axvline(start, **line_args)\n",
    "    ax.axvline(end, **line_args)\n",
    "    ax.axvspan(start, end, facecolor='g', alpha=0.3)\n",
    "\n",
    "# Close-up on a footstep\n",
    "start, end = step_list[4]\n",
    "\n",
    "ax = sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]][start-30:end+30].plot(figsize=(10, 5))\n",
    "ax.axvline(start, **line_args)\n",
    "ax.axvline(end, **line_args)\n",
    "ax.axvspan(start, end, facecolor='g', alpha=0.3)\n",
    "\n",
    "ax = sensor_data.signal[[\"RX\", \"RY\", \"RZ\", \"RV\"]][start-30:end+30].plot(figsize=(10, 5))\n",
    "ax.axvline(start, **line_args)\n",
    "ax.axvline(end, **line_args)\n",
    "ax.axvspan(start, end, facecolor='g', alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On the first two plots.**\n",
    "The repeated patterns (colored in light green) correspond to periods when the foot is moving.\n",
    "During the non-annotated periods, the foot is flat and not moving and the signals are constant.\n",
    "Generally, steps at the beginning and end of the recording, as well as during the u-turn (in the middle of the signal approximatively, see [Protocol](#Protocol-and-equipment)) are a bit different from the other ones.\n",
    "\n",
    "**On the last two plots.** A close-up on a single footstep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comments\n",
    "\n",
    "- In this data set, for a given trial, the signals from the left and right feet are not bundled together. Instead, they produce two different training/testing instances. To recover the signals from both feet and treat them simultaneously, participants need only to find those with the same `.trial_code`. However, there still must be one prediction (step detection) per foot/sensor.\n",
    "\n",
    "- Some metadata (namely `.age`, `.height`, `.weight`, `.bmi` and `.laterality`) can take the value \"NC\" which stands for \"Not Communicated\". This label replaces missing data and depending on the variable may affect up to 2% of the database.\n",
    "\n",
    "- In the whole data set, each `trial_code` value appears exactly twice, because a single trial yields a recording for the right foot and another for the left foot. The concatenation of `trial_code` and `foot` uniquely identifies a recording.\n",
    "\n",
    "- There are uncertainties in the definition of the starts and ends of the steps. Indeed, we can see on previous figures that the start and end could be slightly moved. However, our choice of metric is relatively immune to small variations in the start and end of footsteps.\n",
    "\n",
    "- There is a lot of variability in the step patterns depending on the pathology, the age, the weight, the sensor brand, etc. We invite the participants to skim through the different trials to see how footsteps vary. Generally, long signals (over 40 seconds) display pathological behaviours.\n",
    "\n",
    "- Metadata are provided to help with the step detection task. For instance, one could learn a different model for each pathology group. But it is not compulsory to use them.\n",
    "\n",
    "- For a given `trial_code`, the two associated signals (left foot sensor and right foot sensor) have the same duration (and therefore the same shape). However they might not have the same number of annotated footsteps. Indeed, it often happens that one foot makes one step more compared to the other. Also, between trials, the number of signal samples greatly varies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_avg_min_max(a_list) -> str:\n",
    "    \"\"\"[a_1, a_2,...] -> 'avg (min: minimum, max: maximum)'\"\"\"\n",
    "    return f'{np.mean(a_list):.1f} (min: {np.min(a_list):.1f}, max: {np.max(a_list):.1f})'\n",
    "\n",
    "print(\"On average, in the training set:\")\n",
    "\n",
    "# signal duration\n",
    "sampling_freq = 100 # Hz\n",
    "duration_list = [sensor_data.signal.shape[0]/sampling_freq for sensor_data in data_train]\n",
    "print(f\"- a recording lasts {get_avg_min_max(duration_list)} seconds;\")\n",
    "\n",
    "\n",
    "# Number of steps per recording per foot\n",
    "n_steps_list = [len(step_list) for step_list in labels_train]\n",
    "print(f\"- there are {get_avg_min_max(n_steps_list)} steps per recording per foot;\")\n",
    "\n",
    "# Footstep duration\n",
    "step_duration_list = list()\n",
    "for step_list in labels_train:\n",
    "    step_duration_list.extend(np.diff(step_list).flatten()/sampling_freq)  # list of footstep durations\n",
    "print(f\"- a footstep lasts {get_avg_min_max(step_duration_list)} second;\")\n",
    "\n",
    "# Pathology distribution\n",
    "pathology_counter = Counter(sensor_data.pathology_group for sensor_data in data_train)\n",
    "print(f\"- group distribution: {pathology_counter['Healthy']} 'Healthy', {pathology_counter['Neurological']} 'Neurological', {pathology_counter['Orthopedic']} 'Orthopedic'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "### Performance metric\n",
    "\n",
    "Step detection methods will be evaluated with the **F-score**, based on the following precision/recall definitions.\n",
    "The F-score is first computed per signal then averaged over all instances.\n",
    "\n",
    "- ~~Precision (or positive predictive value). A detected (or predicted) step is counted as correct if its mid-index (mean of its start and end indexes) lies inside an annotated step. The precision is the number of correctly predicted steps divided by the total number of predicted steps.~~\n",
    "\n",
    "- ~~Recall (or sensitivity). An annotated step is counted as detected if its mid-index lies inside a predicted step. The recall is the number of detected annotated steps divided by the total number of annotated steps.~~\n",
    "\n",
    "Precision and recall rely on the \"intersection over union\" metric ($\\text{IoU}$) that measures the overlap of two intervals $[s_1,e_1]$ and $[s_2, e_2]$:\n",
    "\n",
    "$$\n",
    "\\text{IoU}=\\frac{\\big|[s_1,e_1]\\cap [s_2, e_2]\\big|}{\\big|[s_1,e_1]\\cup [s_2, e_2]\\big|}\n",
    "$$\n",
    "\n",
    "- Precision (or positive predictive value). A detected (or predicted) step is counted as correct if it overlaps (measured by $\\text{IoU}$) an annotated step by more than 75%. The precision is the number of correctly predicted steps divided by the total number of predicted steps.\n",
    "\n",
    "- Recall (or sensitivity). An annotated step is counted as detected if it overlaps (measured by $\\text{IoU}$) a predicted step by more than 75%. The recall is the number of detected annotated steps divided by the total number of annotated steps.\n",
    "\n",
    "\n",
    "The F-score is the geometric mean of the precision and recall: $$2\\times\\frac{\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}.$$\n",
    "\n",
    "Note that an annotated step can only be detected once, and a predicted step can only be used to detect one annotated step.\n",
    "If several predicted steps correspond to the same annotated step, all but one are considered as false.\n",
    "Conversely, if several annotated steps are detected with the same predicted step, all but one are considered undetected.\n",
    "\n",
    "**Example 1.**\n",
    "\n",
    "- Annotation (\"ground truth label\"): $\\big[[80, 100], [150, 250], [260, 290]\\big]$ (three steps)\n",
    "- Prediction: $\\big[[80, 98], [105, 120], [256, 295], [298, 310]\\big]$ (four steps)\n",
    "\n",
    "Here, precision is $0.5=(1+0+1+0)/4$, recall is $0.67=(1+0+1)/3$ and the F-score is $0.57$.\n",
    "\n",
    "**Example 2.**\n",
    "\n",
    "- Annotation (\"ground truth label\"): $\\big[[80, 120]\\big]$ (one step)\n",
    "- Prediction: $\\big[[80, 95]\\big]$ (one step)\n",
    "\n",
    "Here, precision is $0=0/1$, recall is $0=0/1$ and the F-score is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple step detection: template matching\n",
    "\n",
    "To illustrate, a simple step detection method is described now.\n",
    "It is based on template matching:\n",
    "- A random step is chosen (the template).\n",
    "- For any new signal, a sliding correlation is computed with this template (dimensions are treated independently). A high correlation indicates a similar pattern.\n",
    "- A peak detection method is applied on the correlation signal.\n",
    "\n",
    "To reduce the computation, only the accelerations are used (angular velocities are discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(7)\n",
    "\n",
    "# choose a sensor recording at random\n",
    "index = rng.choice(len(data_train))\n",
    "sensor_data, step_list = data_train[index], labels_train[index]\n",
    "\n",
    "msg = f\"\"\"Metadata:\n",
    "age\\t\\t{sensor_data.age},\n",
    "gender\\t\\t{sensor_data.gender},\n",
    "height\\t\\t{sensor_data.height},\n",
    "weight\\t\\t{sensor_data.weight},\n",
    "bmi\\t\\t{sensor_data.bmi},\n",
    "laterality\\t{sensor_data.laterality},\n",
    "sensor\\t\\t{sensor_data.sensor},\n",
    "pathology_group\\t{sensor_data.pathology_group},\n",
    "trial code\\t{sensor_data.trial_code},\n",
    "foot\\t\\t{sensor_data.foot}.\"\"\"\n",
    "print(msg)\n",
    "\n",
    "# choose a step template at random\n",
    "step_index = rng.choice(len(step_list))\n",
    "start, end = step_list[step_index]\n",
    "\n",
    "# only keep the acceleration (not the angular velocity)\n",
    "# to reduce computations\n",
    "template = sensor_data.signal[start:end][[\"AX\", \"AY\", \"AZ\", \"AV\"]]\n",
    "template.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliding correlation between signal.AX and template.AX, signal.AY and template.AY, etc. Then max pooling across the four dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import argrelmax\n",
    "\n",
    "score = (sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]]  # select the accelerations\n",
    "         .rolling(template.shape[0], center=True)  # sliding window of the same shape of the template\n",
    "         .apply(lambda x: np.diag(np.corrcoef(x, template, rowvar=False), k=4), raw=True)  # correlation\n",
    "         .max(axis=1)  # max pooling\n",
    "         .fillna(0))  # to remove NaNs at the edges\n",
    "score.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximums of correlation corresponds to detected steps with our template.\n",
    "All that remains is to find them automatically.\n",
    "A simple peak detection heuristic consists in keeping all points that are greater than all samples in a small neighbourhood.\n",
    "In addition, all values below a certain threshold are set to 0, to prevent from detecting steps with low correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "order = 40  # size of the neighbourhood\n",
    "\n",
    "score[score<threshold] = 0.0  # set to 0 all values below a threshold\n",
    "indexes, = argrelmax(score.to_numpy(), order=2*order+1)  # to find local maxima\n",
    "\n",
    "# plot the peaks\n",
    "ax = score.plot()\n",
    "ax.scatter(indexes, score[indexes])\n",
    "\n",
    "# make the prediction\n",
    "# detected steps are centered on the detected peaks and have size 2*order (here 80 samples, not very dynamic).\n",
    "predicted_steps = [[t-order, t+order] for t in indexes]\n",
    "print(\"Detected steps:\")\n",
    "print(predicted_steps)\n",
    "print(f\"({len(predicted_steps)} predicted steps, {len(step_list)} actual steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the F-score on our training example, and then on a testing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import FScoreStepDetection\n",
    "\n",
    "\n",
    "fscore = FScoreStepDetection()([step_list], [predicted_steps])\n",
    "print(f\"(trial code: {sensor_data.trial_code}), foot: {sensor_data.foot}\")\n",
    "print(f\"F-score: {fscore:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing example\n",
    "index = rng.choice(len(data_train))\n",
    "sensor_data, step_list = data_train[index], labels_train[index]\n",
    "\n",
    "score = (sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]]  # select the accelerations\n",
    "         .rolling(template.shape[0], center=True)  # sliding window of the same shape as the template\n",
    "         .apply(lambda x: np.diag(np.corrcoef(x, template, rowvar=False), k=4), raw=True)  # correlations\n",
    "         .max(axis=1)  # max pooling\n",
    "         .fillna(0))  # to remove NaNs at the edges\n",
    "score[score<threshold] = 0.0  # set to 0 all values below a threshold\n",
    "indexes, = argrelmax(score.to_numpy(), order=order)  # to find local maxima\n",
    "predicted_steps = [[t-order, t+order] for t in indexes]\n",
    "\n",
    "fscore = FScoreStepDetection()([step_list], [predicted_steps])\n",
    "print(f\"(trial code: {sensor_data.trial_code}), foot: {sensor_data.foot}\")\n",
    "print(f\"F-score: {fscore:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting to RAMP\n",
    "\n",
    "The example solution presented above is implemented in the sample submission `submissions/starting_kit_1/estimator.py`. The code in this file defines a function called `get_estimator()` that returns a scikit-learn pipeline that performs the same steps as described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelmax\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "rng = np.random.RandomState(7)\n",
    "\n",
    "\n",
    "class Detector(BaseEstimator):\n",
    "    def __init__(self, threshold=0.5, order=40):\n",
    "        self.threshold = threshold\n",
    "        self.order = order\n",
    "        self.step_template = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        assert len(X) == len(\n",
    "            y), f\"Wrong dimensions (len(X): {len(X)}, len(y): {len(y)}).\"\n",
    "\n",
    "        # take a step at random\n",
    "        sensor_data, step_list = rng.choice([*zip(X, y)])\n",
    "        start, end = rng.choice(step_list)\n",
    "        self.step_template = sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]][start:end]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = list()\n",
    "        for sensor_data in X:\n",
    "            score = (sensor_data.signal[[\"AX\", \"AY\", \"AZ\", \"AV\"]]  # select the accelerations\n",
    "                     # sliding window, same shape as the template\n",
    "                     .rolling(self.step_template.shape[0], center=True)\n",
    "                     # correlations\n",
    "                     .apply(lambda x: np.diag(np.corrcoef(x, self.step_template, rowvar=False), k=4))\n",
    "                     # max pooling\n",
    "                     .max(axis=1)\n",
    "                     # to remove NaNs at the edges\n",
    "                     .fillna(0))\n",
    "            # set to 0 all values below a threshold\n",
    "            score[score < self.threshold] = 0.0\n",
    "            # to find local maxima\n",
    "            indexes, = argrelmax(score.to_numpy(), order=self.order)\n",
    "            predicted_steps = [[t - self.order, t + self.order] for t in indexes]\n",
    "            y_pred += [predicted_steps]\n",
    "        return np.array(y_pred, dtype=list)\n",
    "\n",
    "\n",
    "def get_estimator():\n",
    "    # step detection\n",
    "    detector = Detector()\n",
    "\n",
    "    # make pipeline\n",
    "    pipeline = Pipeline(steps=[('detector', detector)])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_estimator()` should return an object (your algorithm) with a `.fit()` and a `.predict()` method, which will be evaluated on a cross-validation.\n",
    "Other files and functions will only be read/executed if they are read/executed by `get_estimator()`.\n",
    "\n",
    "Another (even simpler) example can be found in `submissions/starting_kit/`.\n",
    "\n",
    "In these examples, no real learning or hyperparameter search are performed, but any method will clearly benefit from such procedures. Once you have a submission ready, save your submission code in file named `estimator.py`, which should be stored in a folder (e.g., `my_submission`) within `submissions` (e.g., within `submissions/my_submission/estimator.py`). You can not test your code locally test with:\n",
    "\n",
    "```\n",
    "ramp-test --submission <submission folder name>\n",
    "```\n",
    "\n",
    "where `<submission folder name>` should be the name of the submission folder (e.g., `my_submission`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check list.**\n",
    "- Make sure you have installed ramp-workflow locally (see above)\n",
    "- Make sure that the python file `estimator.py` is in `submissions/<submission folder>`, and the train and test data are in data\n",
    "- If you haven't yet, download the data by executing\n",
    "\n",
    "```\n",
    "python download_data.py\n",
    "```\n",
    "\n",
    "Finally, make sure the local processing goes through by running the\n",
    "```\n",
    "ramp-test --submission <submission folder>\n",
    "```\n",
    "If you want to quickly test the that there are no obvious code errors, use the `--quick-test` flag to only use a small subset of the data (training and testing sets with 5 elements each).\n",
    "\n",
    "```\n",
    "ramp-test --submission <submission folder> --quick-test\n",
    "```\n",
    "\n",
    "See the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ramp",
   "language": "python",
   "name": "ramp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
